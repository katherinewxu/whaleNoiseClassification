import tensorflow_hub as hub
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import librosa
from matplotlib import gridspec
import scipy
from six.moves.urllib.request import urlopen
import io
import soundfile as sf
import librosa
import numpy as np

def noise_reduction(audio, sr, noise_clip, n_fft=2048, hop_length=512, win_length=2048):
    # Calculate the magnitude and phase of the audio signal
    D = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length, win_length=win_length)
    magnitude_signal, phase_signal = librosa.magphase(D)

    # Calculate the magnitude of the noise profile
    D_noise = librosa.stft(noise_clip, n_fft=n_fft, hop_length=hop_length, win_length=win_length)
    magnitude_noise, _ = librosa.magphase(D_noise)

    # Average the noise magnitude over time to get a single noise profile
    magnitude_noise_mean = np.mean(magnitude_noise, axis=1, keepdims=True)

    # Perform spectral subtraction
    magnitude_denoised = magnitude_signal - magnitude_noise_mean
    magnitude_denoised = np.maximum(magnitude_denoised, 0)  # Avoid negative values

    # Reconstruct the audio signal
    D_denoised = magnitude_denoised * phase_signal
    audio_denoised = librosa.istft(D_denoised, hop_length=hop_length, win_length=win_length)

    return audio_denoised
def plot_spectrogram_scipy(signal, sample_rate,
                           hydrophone_sensitivity,
                           title=None,
                           with_colorbar=True
                           ):

    # Compute spectrogram:
    w = scipy.signal.get_window('hann', sample_rate)
    f, t, psd = scipy.signal.spectrogram(signal,
                                         sample_rate,
                                         nperseg=sample_rate,
                                         noverlap=0,
                                         window=w,
                                         nfft=sample_rate,
                                         )
    psd = 10*np.log10(psd) - hydrophone_sensitivity

    # Plot spectrogram:
    plt.imshow(psd, aspect='auto', origin='lower',
               vmin=30, vmax=90,
               cmap='Blues',
               )
    plt.yscale('log')
    y_max = sample_rate / 2
    plt.ylim(10, y_max)

    if with_colorbar:
        plt.colorbar()

    plt.xlabel('Seconds')
    plt.ylabel('Frequency (Hz)')
    plt.title(title or \
              f'Calibrated spectrum levels, 16 {sample_rate / 1000.0} kHz data')

def plot_scores(scores,
                with_steps=False,
                with_dots=True,
                med_filt_size=None,
                ):

    if with_steps:
        # repeat last value to also see a step at the end:
        scores = np.concatenate((scores, scores[-1:]))
        x = range(len(scores))
        plt.step(x, scores, where='post')
    else:
        x = range(len(scores))

    if with_dots:
        plt.plot(x, scores, 'o', color='lightgrey', markersize=9)

    plt.grid(axis='x', color='0.95')
    plt.xlim(xmin=0, xmax=len(scores) - 1)
    plt.ylabel('Model Score')
    plt.xlabel('Seconds')

    if med_filt_size is not None:
        scores_int = [int(s*1000) for s in scores]
        meds_int = scipy.signal.medfilt(scores_int, kernel_size=med_filt_size)
        meds = [m/1000. for m in meds_int]
        plt.plot(x, meds, 'p', color='black', markersize=9)

def plot_results(scores,
                 context_step_samples,
                 signal,
                 sample_rate,
                 hydrophone_sensitivity,
                 title,
                 scores_with_dots=True,
                 scores_with_steps=False,
                 scores_med_filt_size=None,
                 ):

    signal_len_per_scores = len(scores) * context_step_samples

    # Note, the nominal signal length per the obtained score array from the
    # model may be larger than the signal given as input.
    # When this happens, we discard as many trailing scores as necesary:
    while signal_len_per_scores > len(signal):
        scores = scores[:-1]
        signal_len_per_scores = len(scores) * context_step_samples
        # print(f':: adjusting: signal_len_per_scores = {signal_len_per_scores}')

    # A final adjustment to make scores and signal "aligned:"
    signal = signal[:signal_len_per_scores]

    # As numpy array:
    signal = np.array(signal)

    fig = plt.figure(figsize=(24, 8))
    gs = gridspec.GridSpec(2, 1, height_ratios=[1, 1])

    # Plot spectrogram:
    plt.subplot(gs[0])
    plot_spectrogram_scipy(signal, sample_rate, hydrophone_sensitivity,
                           title,
                           with_colorbar=False,
                           )

    # Plot scores:
    fig.add_subplot(gs[1])
    plot_scores(scores,
                with_dots=scores_with_dots,
                with_steps=scores_with_steps,
                med_filt_size=scores_med_filt_size,
                )

    plt.tight_layout()
    plt.show()
# Load the model and get its score_fn for use in the exercise:
MODEL_URL = 'https://tfhub.dev/google/humpback_whale/1'
print(f'== Loading model {MODEL_URL} ...')
model = hub.load(MODEL_URL)
print('model:', model)

score_fn = model.signatures['score']

metadata_fn = model.signatures['metadata']
metadata = metadata_fn()
print('metadata:')
for k, v in metadata.items():
    print(f'  {k}: {v}')

import numpy as np
import librosa
import matplotlib.pyplot as plt
from scipy import signal

def spectral_subtraction(audio, noise_estimate, alpha=2, beta=0.01):
    S_audio = librosa.stft(audio)
    S_noise = librosa.stft(noise_estimate)

    # Ensure noise estimate covers the same length as audio
    if S_noise.shape[1] < S_audio.shape[1]:
        S_noise = np.tile(S_noise, (1, int(np.ceil(S_audio.shape[1] / S_noise.shape[1]))))
    S_noise = S_noise[:, :S_audio.shape[1]]

    P_audio = np.abs(S_audio)**2
    P_noise = np.abs(S_noise)**2
    P_clean = np.maximum(P_audio - alpha * P_noise, beta * P_audio)
    S_clean = np.sqrt(P_clean) * np.exp(1j * np.angle(S_audio))
    audio_clean = librosa.istft(S_clean)
    return audio_clean

def noise_reduction(audio, sample_rate, noise_clip):
    # Apply spectral subtraction
    audio_clean = spectral_subtraction(audio, noise_clip)
    # Apply additional underwater noise removal
    audio_clean = remove_underwater_noise(audio_clean, sample_rate)
    return audio_clean

def remove_underwater_noise(audio, sample_rate):
    """
    Remove common underwater noise sources.

    Args:
    audio (np.array): Input audio signal
    sample_rate (int): Sampling rate of the audio

    Returns:
    np.array: Processed audio with reduced underwater noise
    """
    # High-pass filter to remove low-frequency noise (e.g., flow noise)
    sos = signal.butter(10, 100, 'hp', fs=sample_rate, output='sos')
    audio_filtered = signal.sosfilt(sos, audio)

    # Notch filter to remove specific frequency noise (e.g., boat engine noise)
    notch_freq = 300  # Example frequency, adjust as needed
    quality_factor = 30
    b, a = signal.iirnotch(notch_freq, quality_factor, sample_rate)
    audio_notched = signal.filtfilt(b, a, audio_filtered)

    return audio_notched

def generate_spectrogram(audio, sample_rate, title="Spectrogram"):
    """
    Generate and display a spectrogram of the audio signal.

    Args:
    audio (np.array): Input audio signal
    sample_rate (int): Sampling rate of the audio
    title (str): Title for the spectrogram plot
    """
    plt.figure(figsize=(12, 8))
    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)
    librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='hz')
    plt.colorbar(format='%+2.0f dB')
    plt.title(title)
    plt.tight_layout()
    plt.show()

def visualize_detection(audio, sample_rate, detection_times, detection_scores):
    """
    Visualize whale sound detections on a spectrogram.

    Args:
    audio (np.array): Input audio signal
    sample_rate (int): Sampling rate of the audio
    detection_times (list): List of times (in seconds) where detections occurred
    detection_scores (list): Corresponding detection scores
    """
    plt.figure(figsize=(12, 8))
    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)
    librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='hz')
    plt.colorbar(format='%+2.0f dB')

    for time, score in zip(detection_times, detection_scores):
        plt.axvline(x=time, color='r', linestyle='--', alpha=score)

    plt.title("Whale Sound Detections")
    plt.tight_layout()
    plt.show()
# We will get audio from the 16kHz Pacific Sound data
sample_rate = 16_000

# The particular segment will be from the file corresponding to this day:
year, month, day = 2016, 12, 21

# starting at 00h:25m:
at_hour, at_minute = 0, 25

# and with a 30-min duration:
hours, minutes = 0, 30

# The url to download for that day is:
filename = f'MARS-{year}{month:02}{day:02}T000000Z-16kHz.wav'
url = f'https://pacific-sound-16khz.s3.amazonaws.com/{year}/{month:02}/{filename}'
# Calculate the number of bytes to read
tot_audio_minutes = (at_hour + hours) * 60 + at_minute + minutes
tot_audio_seconds = 60 * tot_audio_minutes
tot_audio_samples = sample_rate * tot_audio_seconds
# Assuming 24-bit audio (3 bytes per sample)
tot_audio_bytes = 3 * tot_audio_samples
# Add some extra bytes for the file header
max_file_size_dl = 1024 + tot_audio_bytes

# Download and load the desired segment of audio
print(f'\n==> Loading segment from {year}-{month}-{day} @ {at_hour}h:{at_minute}m with duration {hours}h:{minutes}m')
psound, _ = sf.read(io.BytesIO(urlopen(url).read(max_file_size_dl)), dtype='float32')

# Get psound_segment from psound based on offset determined by at_hour:at_minute:
offset_seconds = (at_hour * 60 + at_minute) * 60
offset_samples = int(sample_rate * offset_seconds)  # Ensure offset_samples is an integer
psound_segment = psound[offset_samples:]

print(f'len(psound)         = {len(psound)}')
print(f'len(psound_segment) = {len(psound_segment)}')

# The size of psound_segment in seconds as desired is:
psound_segment_seconds = (hours * 60 + minutes) * 60
print(f'psound_segment_seconds = {psound_segment_seconds}')

# Assuming the first few seconds of the audio contain only noise for noise profile extraction
noise_duration_seconds = 5  # Duration of noise sample in seconds
noise_samples = int(sample_rate * noise_duration_seconds)  # Ensure noise_samples is an integer
noise_clip = psound[:noise_samples]

# Apply noise reduction to the segment
psound_segment_denoised = noise_reduction(psound_segment, sample_rate, noise_clip)

# Now psound_segment_denoised can be used for further processing or analysis

# Now, resample our 16 kHz segment as the model expects 10 kHz audio:
print(f'==> Resampling to 10 kHz...')
psound_segment_at_10k = librosa.resample(
            psound_segment,
            orig_sr=sample_rate,
            target_sr=10_000,
            scale=True  # "Scale the resampled signal so that y and y_hat have approximately equal total energy"
            )

# At 10 kHz, this is the nominal size in samples of our desired segment
# when resampled:
psound_segment_samples_at_10k = 10_000 * psound_segment_seconds
print(f'psound_segment_samples_at_10k = {psound_segment_samples_at_10k}')

# Note that psound_segment_at_10k could be a bit larger.
# Do the adjustment so it is our desired segment duration:
psound_segment_at_10k = psound_segment_at_10k[:psound_segment_samples_at_10k]

# Now, we apply the model on our segment at 10 kHz:

# We specify a 1-sec score resolution:
context_step_samples = tf.cast(10_000, tf.int64)

print(f'\n==> Applying model ...')
print(f'   len(psound_segment_at_10k) = {len(psound_segment_at_10k)}')

waveform1 = np.expand_dims(psound_segment_at_10k, axis=1)
waveform_exp = tf.expand_dims(waveform1, 0)  # makes a batch of size 1

psound_scores = score_fn(waveform=waveform_exp,
                         context_step_samples=context_step_samples
                        )
score_values = psound_scores['scores'].numpy()[0]
print(f'==> Model applied.  Obtained score_values = {len(score_values)}')
import matplotlib.pyplot as plt

def plot_spectrogram_with_detections(spectrogram, detections, threshold=0.5, sr=22050, hop_length=512):
    """
    Plot the spectrogram and overlay detected whale song sections.

    Args:
        spectrogram (numpy array): The spectrogram data.
        detections (numpy array): Array of model detection scores (probabilities).
        threshold (float): The threshold to determine detection presence.
        sr (int): Sample rate of the audio.
        hop_length (int): Hop length used to generate the spectrogram.

    Returns:
        None
    """
    # Plot the spectrogram
    plt.figure(figsize=(12, 6))
    plt.imshow(spectrogram, aspect='auto', origin='lower', cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title('Spectrogram with Detections')
    plt.xlabel('Time')
    plt.ylabel('Frequency')

    # Convert detections to binary labels
    detection_mask = (detections >= threshold).astype(int)

    # Overlay detected sections as red vertical lines
    time_steps = np.arange(detections.shape[0]) * hop_length / sr
    for idx, detect in enumerate(detection_mask):
        if detect == 1:
            plt.axvline(x=time_steps[idx], color='red', alpha=0.6)

    plt.show()

# Example usage:
# spectrogram = np.random.rand(100, 100)  # Dummy spectrogram
# detections = np.random.rand(100)  # Dummy detection scores
# plot_spectrogram_with_detections(spectrogram, detections)

# Let's see the results, with a plot of the spectrogram along with
# the 1-sec resolution score and a median filtered score:

print(f'\n==> Plotting results')

title = f'Scores for segment {year}-{month}-{day}'
title += f' @ {at_hour}h:{at_minute}m with duration {hours}h:{minutes}m'
title += ' (resampled to 10 kHz)'

plot_results(score_values,
             context_step_samples,
             signal=psound_segment_at_10k,
             sample_rate=10_000,
             hydrophone_sensitivity=-168.8,
             title=title,
             scores_with_dots=True,
             scores_med_filt_size=25,
            )
import io
from urllib.request import urlopen
import numpy as np
import librosa
import tensorflow as tf
import tensorflow_hub as hub
import soundfile as sf
import matplotlib.pyplot as plt
from scipy import signal
from sklearn.metrics import classification_report

import matplotlib.pyplot as plt

def plot_spectrogram_with_detections(spectrogram, detections, threshold=0.5, sr=22050, hop_length=512):
    """
    Plot the spectrogram and overlay detected whale song sections.

    Args:
        spectrogram (numpy array): The spectrogram data.
        detections (numpy array): Array of model detection scores (probabilities).
        threshold (float): The threshold to determine detection presence.
        sr (int): Sample rate of the audio.
        hop_length (int): Hop length used to generate the spectrogram.

    Returns:
        None
    """
    # Plot the spectrogram
    plt.figure(figsize=(12, 6))
    plt.imshow(spectrogram, aspect='auto', origin='lower', cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title('Spectrogram with Detections')
    plt.xlabel('Time')
    plt.ylabel('Frequency')

    # Convert detections to binary labels
    detection_mask = (detections >= threshold).astype(int)

    # Overlay detected sections as red vertical lines
    time_steps = np.arange(detections.shape[0]) * hop_length / sr
    for idx, detect in enumerate(detection_mask):
        if detect == 1:
            plt.axvline(x=time_steps[idx], color='red', alpha=0.6)

    plt.show()

# Example usage:
# spectrogram = np.random.rand(100, 100)  # Dummy spectrogram
# detections = np.random.rand(100)  # Dummy detection scores
# plot_spectrogram_with_detections(spectrogram, detections)


def spectral_subtraction(audio, noise_estimate, alpha=2, beta=0.01):
    S_audio = librosa.stft(audio)
    S_noise = librosa.stft(noise_estimate)

    # Ensure noise estimate covers the same length as audio
    if S_noise.shape[1] < S_audio.shape[1]:
        S_noise = np.tile(S_noise, (1, int(np.ceil(S_audio.shape[1] / S_noise.shape[1]))))
    S_noise = S_noise[:, :S_audio.shape[1]]

    P_audio = np.abs(S_audio)**2
    P_noise = np.abs(S_noise)**2
    P_clean = np.maximum(P_audio - alpha * P_noise, beta * P_audio)
    S_clean = np.sqrt(P_clean) * np.exp(1j * np.angle(S_audio))
    audio_clean = librosa.istft(S_clean)
    return audio_clean

def remove_underwater_noise(audio, sample_rate):
    sos = signal.butter(10, 100, 'hp', fs=sample_rate, output='sos')
    audio_filtered = signal.sosfilt(sos, audio)
    notch_freq = 300
    quality_factor = 30
    b, a = signal.iirnotch(notch_freq, quality_factor, sample_rate)
    audio_notched = signal.filtfilt(b, a, audio_filtered)
    return audio_notched

def noise_reduction(audio, sample_rate, noise_clip):
    # Apply spectral subtraction
    audio_clean = spectral_subtraction(audio, noise_clip)
    # Apply additional underwater noise removal
    audio_clean = remove_underwater_noise(audio_clean, sample_rate)
    return audio_clean

def generate_spectrogram(audio, sample_rate):
    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)
    return D

def evaluate_model_performance(ground_truth, predictions, threshold=0.5):
    binary_predictions = (predictions >= threshold).astype(int)
    report = classification_report(ground_truth, binary_predictions, target_names=['No Whale', 'Whale'])
    print("Model Performance Report:")
    print(report)

def plot_spectrogram_with_detections(spectrogram, detections, threshold=0.5, sr=22050, hop_length=512):
    plt.figure(figsize=(12, 6))
    plt.imshow(spectrogram, aspect='auto', origin='lower', cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title('Spectrogram with Detections')
    plt.xlabel('Time')
    plt.ylabel('Frequency')

    detection_mask = (detections >= threshold).astype(int)
    time_steps = np.arange(detections.shape[0]) * hop_length / sr
    for idx, detect in enumerate(detection_mask):
        if detect == 1:
            plt.axvline(x=time_steps[idx], color='red', alpha=0.6)

    plt.show()

# Main code
sample_rate = 16_000
year, month, day = 2016, 12, 21
at_hour, at_minute = 0, 25
hours, minutes = 0, 30

filename = f'MARS-{year}{month:02}{day:02}T000000Z-16kHz.wav'
url = f'https://pacific-sound-16khz.s3.amazonaws.com/{year}/{month:02}/{filename}'

tot_audio_minutes = (at_hour + hours) * 60 + at_minute + minutes
tot_audio_seconds = 60 * tot_audio_minutes
tot_audio_samples = sample_rate * tot_audio_seconds
tot_audio_bytes = 3 * tot_audio_samples
max_file_size_dl = 1024 + tot_audio_bytes

print(f'\n==> Loading segment from {year}-{month}-{day} @ {at_hour}h:{at_minute}m with duration {hours}h:{minutes}m')
psound, _ = sf.read(io.BytesIO(urlopen(url).read(max_file_size_dl)), dtype='float32')

offset_seconds = (at_hour * 60 + at_minute) * 60
offset_samples = int(sample_rate * offset_seconds)
psound_segment = psound[offset_samples:]

print(f'len(psound)         = {len(psound)}')
print(f'len(psound_segment) = {len(psound_segment)}')

psound_segment_seconds = (hours * 60 + minutes) * 60
print(f'psound_segment_seconds = {psound_segment_seconds}')

noise_duration_seconds = 5
noise_samples = int(sample_rate * noise_duration_seconds)
noise_clip = psound[:noise_samples]

psound_segment_denoised = noise_reduction(psound_segment, sample_rate, noise_clip)

print(f'==> Resampling to 10 kHz...')
psound_segment_at_10k = librosa.resample(
    psound_segment_denoised,
    orig_sr=sample_rate,
    target_sr=10_000,
    scale=True
)

psound_segment_samples_at_10k = 10_000 * psound_segment_seconds
print(f'psound_segment_samples_at_10k = {psound_segment_samples_at_10k}')

psound_segment_at_10k = psound_segment_at_10k[:psound_segment_samples_at_10k]

context_step_samples = tf.cast(10_000, tf.int64)

# ... (previous code remains the same)

print(f'\n==> Applying model ...')
print(f'   len(psound_segment_at_10k) = {len(psound_segment_at_10k)}')

waveform1 = np.expand_dims(psound_segment_at_10k, axis=1)
waveform_exp = tf.expand_dims(waveform1, 0)

# Cast the waveform to float32
waveform_exp = tf.cast(waveform_exp, tf.float32)

# Assuming you have a loaded model and score_fn
model = hub.load('https://www.kaggle.com/models/google/humpback-whale/TensorFlow2/humpback-whale/1')
score_fn = model.signatures['score']

psound_scores = score_fn(waveform=waveform_exp, context_step_samples=context_step_samples)
score_values = psound_scores['scores'].numpy()[0]
print(f'==> Model applied.  Obtained score_values = {len(score_values)}')

# ... (rest of the code remains the same)

# Generate spectrogram
spectrogram = generate_spectrogram(psound_segment_at_10k, 10_000)

# Plot spectrogram with detections
plot_spectrogram_with_detections(spectrogram, score_values, threshold=0.5, sr=10_000)

# Assuming you have ground truth labels
# y_true = np.array([...])  # Ground truth labels
#evaluate_model_performance(y_true, score_values)

print(f'\n==> Plotting results')

title = f'Scores for segment {year}-{month}-{day}'
title += f' @ {at_hour}h:{at_minute}m with duration {hours}h:{minutes}m'
title += ' (resampled to 10 kHz)'

plot_results(score_values,
             context_step_samples,
             signal=psound_segment_at_10k,
             sample_rate=10_000,
             hydrophone_sensitivity=-168.8,
             title=title,
             scores_with_dots=True,
             scores_med_filt_size=25,
            )
